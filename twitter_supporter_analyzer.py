#!/usr/bin/env python3
import tweepy
import openai
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
import markdown

class TwitterSupporterAnalyzer:
    def __init__(self):
        self.twitter_bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        
        # Twitter API v2 client
        self.twitter_client = tweepy.Client(bearer_token=self.twitter_bearer_token)
        
        # OpenAI client
        from openai import OpenAI
        self.openai_client = OpenAI(api_key=self.openai_api_key)
        
        # æ”¯æ´è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        self.supporter_accounts = self.load_supporter_accounts()
    
    def load_supporter_accounts(self):
        """æ”¯æ´è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            with open('supporter_accounts.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return []
    
    def get_viral_tweets(self, days_back=7):
        """ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†ï¼ˆæœ€æ–°ã‹ã‚‰å„ªå…ˆï¼‰"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days_back)
        
        viral_tweets = []
        
        for account in self.supporter_accounts:
            try:
                # æœ€æ–°ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰å–å¾—ï¼ˆæ™‚ç³»åˆ—é †ï¼‰
                tweets = self.twitter_client.get_users_tweets(
                    id=account['user_id'],
                    start_time=start_time,
                    end_time=end_time,
                    tweet_fields=['public_metrics', 'created_at', 'author_id'],
                    max_results=100  # æœ€æ–°100ä»¶ã‚’å–å¾—
                )
                
                if tweets.data:
                    # æœ€æ–°ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰é †ç•ªã«å‡¦ç†
                    for tweet in tweets.data:
                        metrics = tweet.public_metrics
                        # ãƒã‚¤ãƒ©ãƒ«åˆ¤å®šï¼ˆã„ã„ã­10ä»¥ä¸Š ã¾ãŸã¯ RT5ä»¥ä¸Šï¼‰
                        if metrics['like_count'] >= 10 or metrics['retweet_count'] >= 5:
                            viral_tweets.append({
                                'account_name': account['name'],
                                'username': account['username'],
                                'tweet_id': tweet.id,
                                'text': tweet.text,
                                'created_at': tweet.created_at,
                                'likes': metrics['like_count'],
                                'retweets': metrics['retweet_count'],
                                'replies': metrics['reply_count'],
                                'url': f"https://twitter.com/{account['username']}/status/{tweet.id}"
                            })
                        
            except Exception as e:
                print(f"Error fetching tweets for {account['name']}: {e}")
                continue
        
        # æœ€æ–°ã®æŠ•ç¨¿é †ã«ã‚½ãƒ¼ãƒˆï¼ˆä½œæˆæ—¥æ™‚ãŒæ–°ã—ã„é †ï¼‰
        return sorted(viral_tweets, key=lambda x: x['created_at'], reverse=True)
    
    def filter_relevant_tweets(self, tweets):
        """å±±ç”°å¤ªéƒè­°å“¡é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        yamada_keywords = [
            'å±±ç”°å¤ªéƒ', 'è¡¨ç¾ã®è‡ªç”±', 'è‘—ä½œæ¨©', 'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼', 'DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«',
            'å…ç«¥ãƒãƒ«ãƒ', 'å…ãƒ', 'éå®Ÿåœ¨', 'è¡¨ç¾è¦åˆ¶', 'CODA', 'TPP',
            'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'ã‚¢ãƒ‹ãƒ¡', 'ãƒãƒ³ã‚¬', 'ã‚²ãƒ¼ãƒ ', 'åŒäºº', 'ã‚ªã‚¿ã‚¯',
            'ITæ”¿ç­–', 'ãƒ‡ã‚¸ã‚¿ãƒ«åº', 'ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼', 'ã‚µã‚¤ãƒãƒ¼', 'AIè¦åˆ¶',
            'å‚è­°é™¢', 'è‡ªæ°‘å…š', 'æ”¿æ²»', 'è­°å“¡', 'æ”¿ç­–', 'æ³•æ¡ˆ'
        ]
        
        relevant_tweets = []
        for tweet in tweets:
            text = tweet['text'].lower()
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            if any(keyword.lower() in text for keyword in yamada_keywords):
                tweet['relevance_score'] = sum(1 for keyword in yamada_keywords if keyword.lower() in text)
                relevant_tweets.append(tweet)
        
        # é–¢é€£åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        relevant_tweets.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        # é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆãŒãªã„å ´åˆã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return relevant_tweets if relevant_tweets else tweets

    def analyze_tweets_with_ai(self, tweets):
        """AIã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åˆ†æãƒ»è¦ç´„"""
        if not tweets:
            return "æœ¬æ—¥ã¯ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_tweets = self.filter_relevant_tweets(tweets)
        
        # ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’æ•´ç†
        tweet_texts = []
        for tweet in filtered_tweets[:20]:  # ä¸Šä½20ä»¶ã‚’åˆ†æ
            relevance = f"(é–¢é€£åº¦: {tweet.get('relevance_score', 0)})" if 'relevance_score' in tweet else ""
            tweet_texts.append(f"@{tweet['username']}: {tweet['text']} (ğŸ‘{tweet['likes']} ğŸ”„{tweet['retweets']}) {relevance}")
        
        prompt = f"""ä»¥ä¸‹ã¯å±±ç”°å¤ªéƒè­°å“¡ã«é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã§ã™ã€‚
åˆ†é‡ã”ã¨ã«æ•´ç†ã—ã¦ã€æ·¡ã€…ã¨ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

## åˆ†é‡åˆ¥åˆ†æï¼ˆè©²å½“ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰

### ğŸ“ è¡¨ç¾ã®è‡ªç”±ãƒ»è¦åˆ¶é–¢é€£
- è©²å½“ãƒ„ã‚¤ãƒ¼ãƒˆã®è¦ç´„

### ğŸ’» ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»ITæ”¿ç­–
- è©²å½“ãƒ„ã‚¤ãƒ¼ãƒˆã®è¦ç´„

### ğŸ¨ ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãƒ»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- è©²å½“ãƒ„ã‚¤ãƒ¼ãƒˆã®è¦ç´„

### âš–ï¸ æ³•æ¡ˆãƒ»æ”¿ç­–ææ¡ˆ
- è©²å½“ãƒ„ã‚¤ãƒ¼ãƒˆã®è¦ç´„

### ğŸ—³ï¸ æ”¿æ²»æ´»å‹•ãƒ»é¸æŒ™
- è©²å½“ãƒ„ã‚¤ãƒ¼ãƒˆã®è¦ç´„

### ğŸ“Š ãã®ä»–
- ãã®ä»–ã®è©±é¡Œ

ãƒ„ã‚¤ãƒ¼ãƒˆä¸€è¦§ï¼š
{chr(10).join(tweet_texts)}

å„åˆ†é‡ã«ã¤ã„ã¦ã€è©²å½“ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿è¨˜è¼‰ã—ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"
    
    def generate_report(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†ä¸­...")
        viral_tweets = self.get_viral_tweets()
        
        print(f"{len(viral_tweets)}ä»¶ã®ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç™ºè¦‹")
        
        # é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        relevant_tweets = self.filter_relevant_tweets(viral_tweets)
        print(f"å±±ç”°å¤ªéƒè­°å“¡é–¢é€£: {len(relevant_tweets)}ä»¶")
        
        print("AIåˆ†æã‚’å®Ÿè¡Œä¸­...")
        analysis = self.analyze_tweets_with_ai(viral_tweets)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_date = datetime.now().strftime('%Y-%m-%d')
        report_content = f"""# å±±ç”°å¤ªéƒè­°å“¡é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
## {report_date}

### ğŸ“Š ã‚µãƒãƒªãƒ¼
- ğŸ” ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆç·æ•°: {len(viral_tweets)}ä»¶
- ğŸ¯ é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆ: {len(relevant_tweets)}ä»¶
- ğŸ“ˆ é–¢é€£åº¦ã®é«˜ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å„ªå…ˆè¡¨ç¤º

{analysis}

## ğŸ”¥ é–¢é€£åº¦ã®é«˜ã„ãƒã‚¤ãƒ©ãƒ«ãƒ„ã‚¤ãƒ¼ãƒˆï¼ˆè©³ç´°ï¼‰
"""
        
        # é–¢é€£åº¦ã®é«˜ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å„ªå…ˆè¡¨ç¤º
        display_tweets = relevant_tweets[:10] if relevant_tweets else viral_tweets[:10]
        for tweet in display_tweets:
            relevance_info = f"- **ğŸ¯ é–¢é€£åº¦**: {tweet.get('relevance_score', 0)}ç‚¹\n" if 'relevance_score' in tweet else ""
            report_content += f"""
### [{tweet['account_name']}](https://twitter.com/{tweet['username']})
- **ãƒ„ã‚¤ãƒ¼ãƒˆ**: {tweet['text']}
- **ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ**: ğŸ‘{tweet['likes']} ğŸ”„{tweet['retweets']} ğŸ’¬{tweet['replies']}
{relevance_info}- **URL**: {tweet['url']}
- **æŠ•ç¨¿æ™‚åˆ»**: {tweet['created_at']}

---
"""
        
        # reportsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('reports', exist_ok=True)
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        html_content = markdown.markdown(report_content)
        
        with open(f'reports/report_{report_date}.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        with open(f'reports/report_{report_date}.html', 'w', encoding='utf-8') as f:
            f.write(f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ”¯æ´è€…ãƒ„ã‚¤ãƒ¼ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ - {report_date}</title>
    <style>
        body {{ font-family: 'Hiragino Sans', 'Yu Gothic', sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        .tweet {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 8px; }}
    </style>
</head>
<body>
    {html_content}
</body>
</html>""")
        
        print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: reports/report_{report_date}.html")
        return report_content

if __name__ == "__main__":
    analyzer = TwitterSupporterAnalyzer()
    analyzer.generate_report()